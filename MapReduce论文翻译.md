# MapReduce：大型集群上的简化数据处理  [🔗][1]

---

## 摘要

MapReduce是一个编程模型，也是一个处理和生成超大数据集的算法模型的相关实现。用户定义map函数来处理key/value键值对来产生一系列的中间的key/value键值对。然后再创建一个*reduce*函数用来合并所有的具有相同中间key值的中间value值。现实世界中有很多满足上述处理模型的例子，本论文将详细描述这个模型。

MapReduce架构的程序能够在大量的普通配置的计算机上实现并行化处理。这个系统在运行时只关心：如何分割输入数据、在大量计算机组成的集群上的调度、集群中计算机的错误处理以及管理集群中计算机之间必要的通信。采用MapReduce架构可以使那些没有并行计算和分布式处理系统开发经验的程序员有效利用分布式系统的丰富资源。

我们的MapReduce实现运行在一个由大的商业机构成的集群当中并且是高度可扩展的：一个典型的MapReduce计算往往由几千台机器组成、处理以TB计算的数据。程序员会觉得这个系统非常好用：已经有成千上万的MapReduce程序被实现出来并且每天有上千个MapReduce任务运行在Google的集群上。

## 1 Introduction（介绍）

在过去的5年里，包括本文作者在内的Google的很多程序员，为了处理海量的原始数据（rawdata），已经实现了数以百计的、专用的计算方法。这些计算方法用来处理大量的原始数据，比如，文档抓取（类似网络爬虫的程序）、Web请求日志等等；也为了计算处理各种类型的衍生数据（deriveddata），比如倒排索引、Web文档的图结构的各种表示形式、每台主机上网络爬虫抓取的页面数量的汇总、每天被请求的最多的查询的集合等等。大多数这样的数据处理运算在概念上很容易理解。然而由于输入的数据量巨大，因此要想在可接受的时间内完成运算，只有将这些计算分布在成百上千的主机上。如何处理并行计算、如何分发数据、如何处理错误？所有这些问题综合在一起，需要大量的代码处理，因此也使得原本简单的运算变得难以处理。

为了解决上述复杂的问题，我们设计一个新的抽象模型，使用这个抽象模型，我们只要表述我们想要执行的简单运算即可，而不必关心并行计算、容错、数据分布、负载均衡等复杂的细节，这些问题都被封装在了一个库里面。设计这个抽象模型的灵感来自Lisp和许多其他函数式语言的*map*和*reduce*的原语。我们意识到我们大多数的运算都包含这样的操作：在输入数据的逻辑“记录”上应用*map*操作得出一个中间key/value集合，然后在所有具有相同key值的value值上应用*reduce*操作，从而达到合并中间的数据，得到一个想要的结果的目的。使用MapReduce模型，再结合用户实现的*map*和*reduce*函数，我们就可以非常容易的实现大规模并行化计算；通过MapReduce模型自带的“再次执行”（re-execution）功能，也提供了初级的容灾实现方案。

这个工作（实现一个MapReduce框架模型）的主要贡献是通过简单的接口来实现自动的并行化和大规模的分布式计算，同时该接口的实现能在大型的商用PC集群上获得非常高的性能。

第二部分描述基本的编程模型和一些使用案例。第三部分描述了为我们的基于集群的计算环境量身定做的MapReduce接口。第四部分描述了一些我们认为对于编程模型有用的的改进。第五部分是对我们的实现在不同任务下的性能测试。第六部分包含了MapReduce在Google内的使用情况，包括我们以它为基础重写我们的产品索引系统的经验。第七部分讨论了相关的工作以及未来的发展。

## 2Programming Model（编程模型）

MapReduce编程模型的原理是：利用key/value输入集合来产生key/value输出集合。MapReduce库的用户用两个函数表达这个计算：*map*和*reduce*。

用户自定义的*map*函数接受一个输入的key/value值，然后产生一个中间key/value值的集合。MapReduce库把所有具有相同中间key值*I*的中间value值集合在一起后传递给*reduce*函数。

用户自定义的*reduce*函数接受一个中间key的值*I*和相关的一个value值的集合。*reduce*函数合并这些value值，形成一个较小的value值的集合。一般的，每次*reduce*函数调用只产生0或1个输出value值。通常我们通过一个迭代器把中间value值提供给*reduce*函数，这样我们就可以处理无法全部放入内存中的大量的value值的集合。

### 2.1 Example（例子）

例如，计算一个大的文档集合中每个单词出现的次数，下面是伪代码段：

```C++
map(String key, String value):
   // key: document name
   // value: document contents
    for each word w in value:
        EmitIntermediate(w,"1");
reduce(String key, Iterator values):
   // key: a word
   // values: a list of counts
    int result = 0;
    for each v in values:
        result += ParseInt(v);
    Emit(AsString(result));
```

*map*函数为在每一个单词出现的时候，为它加上一个计数（在这个简单的例子中就是加1）。*reduce*函数对每个单词的所有计数进行叠加。

另外，用户需要用输入输出文件的名字，以及可选的tuning parameters去填充一个叫*mapreduce specification*的对象。之后，用户调用*MapReduce*函数，将定义的上述对象传递进去。用户的代码将和MapReduce库相连（由C++实现）。Appendix A中有这个例子所有的代码文档。

### 2.2 Types（类型）

虽然在上述的伪代码中输入输出都是字符串类型的，但事实上，用户提供的*map*和*reduce*函数都是有相关联的类型的：
$$
\begin{align}
&\Large map  \qquad\quad (k_1, v_1) → list(k_2, v_2) \\
&\Large reduce \qquad (k_2, list(v_2)) → list(v_2)
\end{align}
$$
需要注意的是，输入的key和value与输出的key和value是不同的类型，而中间的key和value与输出的key和value是相同的类型。

我们的C++中使用字符串类型作为用户自定义函数的输入输出，用户在自己的代码中对字符串进行适当的类型转换。

### 2.3 More Examples（更多的例子）

这里还有一些有趣的简单例子，可以很容易的使用MapReduce模型来表示：

**分布式的Grep：** *Map*函数获取匹配提供的模式的行，*Reduce*函数只是简单地将这些中间数据拷贝到输出。

**计算URL访问频率：** *Map*函数处理日志中web页面请求的记录，然后输出*<URL,1>*。Reduce函数把相同URL的value值都累加起来，产生*<URL, total count>*结果。

**倒转网络链接图：** *Map*函数在源页面（source）中搜索所有的链接目标（target）并输出为*<target, source>*。*Reduce*函数把给定链接目标（target）的链接组合成一个列表，输出*<target, list(source)>*。

**每个主机的检索词向量(Term-Vector per Host)：**一个term vector表示一系列*<word,frequency>*的键值对，word表示一篇或者一系列文章中出现的比较重要的单词，frequency表示它们出现的次数。 *Map*函数对于每篇输入的文章输出*<hostname,term vector>*键值对（其中hostname是从文章所在的URL中抽取出来的）*Reduce*函数获取给定host的term vectors。它将这些term vectors累加起来，丢弃非频繁出现的term，并产生一个最终的*<hostname,term vector>*对。

**倒排索引(Inverted Index)：** *Map*函数对每篇文章进行处理，并输出一系列的*<word, document ID>*对。*Reduce*函数接收给定word的所有键值对，对相应的document ID进行排序并且输出*<word, list(document ID)>*对。所有输出对的集合构成了一个简单的倒排索引。用了 MapReduce模型，对单词位置的追踪就变得非常简单了。

**分布式排序：** *Map*函数从每个record中抽取出key，产生*<key, record>*键值对。*Reduce*函数只是简单地将所有对输出。这个计算模型依赖于Section 4.1中描述的划分技巧以及Section 4.2中描述的排序特性。

## 3 Implementation（实现）

MapReduce模型有多种接口实现方式。如何正确选择取决于具体的环境。例如，一种实现方式适用于小型的共享内存方式的机器，另外一种实现方式则适用于大型NUMA架构的多处理器的主机，而有的实现方式更适合大型的网络连接集群。

本节中描述的实现基于的是Google中最常用的计算环境：一个由大量商用PC机通过交换以太网互联的集群。在我们的环境中：

1. x86架构、运行Linux操作系统、双处理器、2-4 GB内存的机器。
2. 普通的网络硬件设备，每个机器的带宽为百兆或者千兆，但是远小于网络的平均带宽。
3. 集群中包含成百上千的机器，因此，机器故障是常态。
4. 存储为廉价的内置IDE硬盘。一个内部分布式文件系统用来管理存储在这些磁盘上的数据。文件系统通过数据复制来在不可靠的硬件上保证数据的可靠性和有效性。
5. 用户提交工作(job)给调度系统。每个工作(job)都包含一系列的任务(task)，调度系统将这些任务调度到集群中多台可用的机器上。

### 3.1 Execution Overview（执行概述）

通过将输入数据自动分割成$M$份，*Map*函数得以在多台机器上分布式执行。每一个输入块都能并行地在不同的机器上执行。通过划分函数（例如，*hash(key) mod R*）将中间键划分为$R$份，*Reduce*函数也能被分布式地调用。其中划分的数目$R$和划分函数都是由用户指定的。

![execution_overview.PNG][2]

上图1展示了在我们的实现中MapReduce全部的流程。当用户程序调用*MapReduce*函数时，接下来的动作将按序发生（图1中标记的数字与下面的数字是对应的）：

1. 用户程序首先调用的MapReduce库将输入文件分成$M$个数据片度，每个数据片段的大小一般从16MB到64MB(可以通过可选的参数来控制每个数据片段的大小)。然后用户程序在机群中创建大量的程序副本。
2. 这些程序副本中的有一个特殊的程序：**master**。副本中其它的程序都是**worker**程序，由master分配任务。有$M$个Map任务和$R$个Reduce任务将被分配，master将一个Map任务或Reduce任务分配给一个空闲的worker。
3. 被分配到Map任务的worker会去读取相应的输入块的内容。它从输入文件中解析出key/value对并且将每个key/value对传送给用户定义的*map*函数。而由*map*函数产生的中间key/value对缓存在内存中。
4. 被缓存的key/value对会阶段性地写回本地磁盘，并且被划分函数分割成$R$份。这些缓存对在磁盘上的位置会被回传给master，master再负责将这些位置转发给Reduce worker。
5. 当Reduce worker从master那里接收到这些位置信息时，它会使用远程过程调用从Map worker的本地磁盘中获取缓存的数据。当Reduce worker读入全部的中间数据之后，它会根据中间键对它们进行排序，这样所有具有相同键的键值对就都聚合在一起了。排序是必须的，因为会有许多不同的key被映射到同一个Reduce任务中。如果中间数据的数量太大，以至于不能够装入内存的话，那么就要在外部进行排序。
6. Reduce worker程序遍历排序后的中间数据，对于每一个唯一的中间key值，Reduce worker程序将这个key值和它相关的中间value值的集合传递给用户自定义的*Reduce*函数。*Reduce*函数的输出被追加到所属分区的输出文件。
7. 当所有的Map和Reduce任务都完成之后，master唤醒用户程序。在这个时候，在用户程序里的对*MapReduce*调用才返回。

在成功完成任务之后，MapReduce的输出存放在$R$个输出文件中（对应每个Reduce任务产生一个输出文件，文件名由用户指定）。一般情况下，用户不需要将这$R$个输出文件合并成一个文件，因为他们经常把这些文件作为另外一个MapReduce的输入，或者在另外一个可以处理多个分割文件的分布式应用中使用。

### 3.2 Master Data Structures（Master数据结构）

在master中保存了许多的数据结构。对于每个Map任务和Reduce任务，master都保存了它们的状态（*idle, in-progress or completed*）以及worker所在机器的标识（对于非idle状态的任务而言）。
master相当于是一个管道，Map任务所产生的中间文件通过它被传递给了Reduce任务。因此，对于每一个已经完成的Map任务，master会存储由它产生的$R$个中间文件的位置和大小。当Map任务完成的时候，master就会收到位置和大小的更新信息。而这些信息接下来就会逐渐被推送到处于*in-progress*状态的Reduce任务中。

### 3.3 Fault Tolerance（容错）

因为MapReduce库的设计初衷是用成千上万的机器去处理大量的数据，所以它就必须能用优雅的方式对机器故障进行处理。

#### Worker Failure

master周期性地ping每个worker。如果在一个约定的时间范围内没有收到worker返回的信息，master将把这个worker标记为failed。所有由这个失效的worker完成的map任务被回退为初始的*idle*状态，之后这些任务就可以被安排给其他worker。同样的，worker失效时正在运行的Map或Reduce任务也将被重新置为*idle*状态，等待重新调度。

当worker故障时，由于已经完成的Map任务的输出存储在本地磁盘上，Map任务的输出已不可访问了，因此必须重新执行。而已经完成的Reduce任务的输出存储在全局文件系统上，因此不需要再次执行。

当一个Map任务首先被worker A执行，之后由于worker A失效了又被调度到worker B执行，这个“重新执行”的动作会被通知给所有执行Reduce任务的worker。任何还没有从worker A读取数据的Reduce任务将从worker B读取数据。

MapReduce可以处理大规模worker失效的情况。比如，在一个MapReduce操作执行期间，在正在运行的集群上进行网络维护引起80台机器在几分钟内不可访问了，MapReduce的master只是简单地将不可达的worker机器上的工作重新执行了一遍，接着再继续往下执行，最终完成了MapReduce的操作。

#### Master Failure

对于master，我们可以简单地对上文所述的master数据结构做周期性的快照。如果一个master任务失效，我们可以很快地根据最新的快照来重新启动一个master task。但是，因为我们只有一个master，因此故障的概率比较低。所以，在我们的实现中如果master出现了故障就只是简单地停止MapReduce操作。用户可以检测到这种情况，并且如果他们需要的话可以重新开始一次MapReduce操作。

#### Semantics in the Presence of Failures（在失效方面的处理机制）  

当用户提供的*map*和*reduce*操作是输入确定性函数（即相同的输入产生相同的输出）时，我们的分布式实现在任何情况下的输出都和所有程序没有出现错误、顺序执行时产生的输出是一样的。

我们依赖Map任务和Reduce任务**原子性地提交输出**来实现上述特性。每个工作中的任务把它的输出写到私有的临时文件中。每个Reduce任务生成一个这样的文件，而每个Map任务则生成$R$个这样的文件（一个Reduce任务对应一个文件）。当一个Map任务完成的时侯，worker就会发送一个包含$R$个临时文件名的完成消息给master，master将这R个文件的名字记录在数据结构里。如果master从一个已经完成的Map任务再次接收到到一个完成消息，master将忽略这个消息。

当Reduce任务完成时，Reduce worker进程以原子的方式把临时文件重命名为最终的输出文件。如果同一个Reduce任务在多台机器上执行，针对同一个最终的输出文件将有多个重命名操作执行。我们依赖底层文件系统提供的重命名操作的原子性来保证最终的文件系统状态仅仅包含一个Reduce任务产生的数据。

使用MapReduce模型的程序员可以很容易地理解他们程序的行为，因为我们绝大多数的Map和Reduce操作是确定性的，而且存在这样的一个事实：我们的失效处理机制等价于一个顺序的执行的操作。当Map或Reduce操作是不确定性的时候，我们提供虽然较弱但是依然合理的处理机制。当使用非确定操作的时候，一个Reduce任务$R_1$的输出等价于一个非确定性程序顺序执行产生时的输出。并且，另一个Reduce任务的$R_2$输出符合另一个非确定程序顺序执行产生的输出$R_2$。

考虑Map任务$M$和Reduce任务$R_1$、$R_2$的情况。我们设定$e(R_i)$是$R_i$已经提交的执行结果（有且仅有一个这样的执行过程）。当$e(R_1)$读取了由$M$一次执行产生的输出，而$e(R_2)$读取了由$M$的另一次执行产生的输出，导致了较弱的失效处理。

### 3.4 Locality（存储位置）

在我们的计算运行环境中，网络带宽是一个相当匮乏的资源。我们通过将输入数据（由GFS管理）存储在集群中每台机器的本地磁盘的方法来节省带宽。GFS将输入文件切分成64 MB大小的块，并且将每个块的多份拷贝（通常为3份）存储在不同的机器上。MapReduce的master获取所有输入文件的位置信息，然后尽量将一个Map任务调度在包含相关输入数据拷贝的机器上执行。当发生故障时，再将Map任务调度到邻近的具有该任务输入文件副本的机器（即在同一台交换机内具有相同数据的机器）。**当在一个集群的大量机器上做MapReduce操作时，大多数的输入数据都是从本地读取的，而不用消耗带宽。**

### 3.5 Task Granularity（任务粒度）

如上所述，我们将Map操作分成$M$份，Reduce操作分成$R$份。理想情况下，$M$和$R$应当比集群中worker的机器数量要多得多。在每台worker机器都执行大量的不同任务能够提高集群的动态的负载均衡能力，并且能够加快故障恢复的速度：失效机器上执行的大量Map任务都可以分布到所有其他的worker机器上去执行。

但是实际上，在我们的具体实现中对$M$和$R$的取值都有一定的客观限制，因为master必须执行$O(M+R)$次调度，并且在内存中保存$O(M*R)$个状态（但是内存使用的常数还是比较小的，$O(M*R)$个Map/Reduce任务状态对，每个的大小大概在一个字节）。

更进一步，$R$值通常是由用户指定的，因为每个Reduce任务最终都会生成一个独立的输出文件。实际使用时我们也倾向于选择合适的$M$值，以使得每一个独立任务都是处理大约16M到64M的输入数据（这样，上面描写的输入数据本地存储优化策略才最有效）。另外，我们把$R$值设置为worker机器数量的较小倍数。我们通常会用这样的比例来执行MapReduce：M=200000，R=5000，使用2000台worker机器。

### 3.6 Backup Tasks（备用任务）

影响一个MapReduce的总执行时间最通常的因素是“落伍者”：在运算过程中，如果有一台机器花了很长的时间才完成最后几个Map或Reduce任务，导致MapReduce操作总的执行时间超过预期。出现“落伍者”的原因非常多。比如：如果一个机器的硬盘出了问题，在读取的时候要经常的进行读取纠错操作，导致读取数据的速度从30 M/s降低到1 M/s。如果cluster的调度系统在这台机器上又调度了其他的任务，由于CPU、内存、本地硬盘和网络带宽等竞争因素的存在，导致执行MapReduce代码的执行效率更加缓慢。我们最近遇到的一个问题是由于机器的初始化代码有bug，导致关闭了的处理器的缓存：在这些机器上执行任务的性能和正常情况相差上百倍。

我们有一个通用的机制来减少“落伍者”出现的情况。当一个MapReduce操作接近完成的时候，master调度备用（backup）任务进程来执行剩下的、处于处理中状态（*in-progress*）的任务。无论是最初的执行进程、还是备用（backup）任务进程完成了任务，我们都把这个任务标记成为已经完成。我们调优了这个机制，通常只会占用比正常操作多几个百分点的计算资源。我们发现采用这样的机制对于减少超大MapReduce操作的总处理时间效果显著。例如，在5.3节描述的排序任务，在关闭掉备用任务的情况下要多花44%的时间完成排序任务。

## 4 Refinements（技巧）

虽然简单的Map和Reduce函数提供的基本功能已经能够满足大部分的计算需要，我们还是发掘出了一些有价值的扩展功能。本节将描述这些扩展功能。

### 4.1 Partitioning Function（分区函数）

MapReduce的使用者通常会指定Reduce任务和Reduce任务输出文件的数量（*R*）。我们在中间key上使用分区函数来对数据进行分区，之后再输入到后续任务执行进程。一个缺省的分区函数是使用hash方法（比如，*hash(key) mod R*）进行分区。hash方法能产生非常平衡的分区。然而有的其它的一些分区函数对key值进行的分区将非常有用。比如输出的key值是URLs，我们希望每个主机的所有条目保持在同一个输出文件中。为了支持类似的情况，MapReduce库的用户需要提供专门的分区函数。例如，使用*“hash(Hostname(urlkey)) **mod** R”*作为分区函数就可以把所有来自同一个主机的URLs保存在同一个输出文件中。

### 4.2 Ordering Guarantees（顺序保证）

我们确保在给定的分区中，对中间key/value键值对数据的处理是按照key值增序处理的。这样的处理顺序确保了每一个划分产生一个排好序的输出文件。这样对于输出文件，如果需要支持根据key进行有效的随机查找会比较方便。同时，输出的用户也会觉得已经排好序的数据使用起来特别方便。

### 4.3 Combiner Function（Combiner函数）

在某些情况下，*Map*函数产生的中间key值的重复数据会占很大的比重，并且，用户自定义的*Reduce*函数满足结合律和交换律。在2.1节的词数统计程序是个很好的例子。由于词频率倾向于一个Zipf分布(齐夫分布)，每个Map任务将产生成千上万个这样的记录*<the, 1>*。所有的这些记录将通过网络被发送到一个单独的Reduce任务，然后由这个Reduce任务把所有这些记录累加起来产生一个数字。我们允许用户指定一个可选的*Combiner*函数，*Combiner*函数首先在本地将这些记录进行一次合并，然后将合并的结果再通过网络发送出去。

*Combiner*函数在每台执行Map任务的机器上都会被执行一次。一般情况下，*Combiner*和*Reduce*函数是一样的。*Combiner*函数和*Reduce*函数之间唯一的区别是MapReduce库怎样控制函数的输出。*Reduce*函数的输出被保存在最终的输出文件里，而*Combiner*函数的输出被写到中间文件里，然后被发送给Reduce任务。

部分的合并中间结果可以显著的提高一些MapReduce操作的速度。附录A包含一个使用*Combiner*函数的例子。

### 4.4 Input and Output Types（输入输出支持的类型）

MapReduce库支持几种不同的格式的输入数据。比如，文本模式的输入数据的每一行被视为是一个key/value键值对。key是文件的偏移量，value是那一行的内容。另外一种常见的格式是以key进行排序来存储的key/value序列。每种输入类型的实现都必须能够把输入数据分割成数据片段，该数据片段能够由单独的Map任务来进行后续处理（例如，文本模式的范围分割必须确保仅仅在每行的边界进行范围分割）。虽然大多数MapReduce的使用者仅仅使用很少的预定义输入类型就满足要求了，但是使用者依然可以通过提供一个简单的*Reader*接口实现就能够支持一个新的输入类型。

*Reader*并非一定要从文件中读取数据，比如，我们可以很容易的实现一个从数据库里读记录的*Reader*，或者从内存中的数据结构读取数据的*Reader*。

类似的，我们提供了一些预定义的输出数据的类型，通过这些预定义类型能够产生不同格式的数据。用户采用类似添加新的输入数据类型的方式增加新的输出类型。

### 4.5 Side-effects（副作用）

有些情况下，MapReduce的使用者发现，在Map或Reduce操作过程中增加辅助的输出文件会比较省事。我们依靠程序writer把这种“副作用”变成原子的和幂等的*（注：幂等的指一个总是产生相同结果的数学运算）*。通常应用程序首先把输出结果写到一个临时文件中，在输出全部数据之后，在使用系统级的原子操作重新命名这个临时文件。

如果一个任务产生了多个输出文件，我们没有提供类似两阶段提交的原子操作支持这种情况。因此，对于会产生多个输出文件、并且对于跨文件有一致性要求的任务，都必须是确定性的任务。但是在实际应用过程中，这个限制还没有给我们带来过麻烦。

### 4.6 Skipping Bad Records（跳过损坏的记录）

有时候，用户程序中的bug导致*Map*或者*Reduce*函数在处理某些记录的时候崩溃，MapReduce操作无法顺利完成。惯常的做法是修复bug后再次执行MapReduce操作，但是，有时候找出这些bug并修复它们不是一件容易的事情；这些bug也许是在第三方库里边，而我们手头没有这些库的源代码。而且在很多时候，忽略一些有问题的记录也是可以接受的，比如在一个巨大的数据集上进行统计分析的时候。我们提供了一种可选的执行模式，当MapReduce库检测到一些记录会造成崩溃时，就会主动跳过它们，从而保证正常地运行。 

每个worker进程都设置了信号处理函数捕获内存段异常（segmentation violation）和总线错误（bus error）。在执行*Map*或者*Reduce*操作之前，MapReduce库通过全局变量保存记录序号。如果用户程序触发了一个系统信号，消息处理函数将用“最后一口气”通过UDP包向master发送处理的最后一条记录的序号。当master看到在处理某条特定记录不止失败一次时，master就标志着条记录需要被跳过，并且在下次重新执行相关的Map或者Reduce任务的时候跳过这条记录。

### 4.7 Local Execution（本地执行）

调试*Map*和*Reduce*函数的bug是非常困难的，因为实际执行操作时不但是分布在系统中执行的，而且通常是在好几千台计算机上执行，具体的执行位置是由master进行动态调度的，这又大大增加了调试的难度。为了简化调试、分析和小规模测试，我们开发了一套MapReduce库的本地实现版本，通过使用本地版本的MapReduce库，MapReduce操作在本地计算机上顺序的执行。用户可以控制MapReduce操作的执行，可以把操作限制到特定的Map任务上。用户利用指定的flag启动程序，然后就能非常简单地使用任何它们觉得有用的调试或者测试工具了（比如gdb）。

### 4.8 Status Information（状态信息）

master运行了一个内置的HTTP server并且暴露了一系列状态信息页面以供使用。用户可以监控各种执行状态。状态信息页面显示了包括计算执行的进度，比如已经完成了多少任务、有多少任务正在处理、输入的字节数、中间数据的字节数、输出的字节数、处理百分比等等。页面还包含了指向每个任务的stderr和stdout文件的链接。用户根据这些数据预测计算需要执行大约多长时间、是否需要增加额外的计算资源。这些页面也可以用来分析什么时候计算执行的比预期的要慢。

另外，处于最顶层的状态页面显示了哪些worker失效了，以及他们失效的时候正在运行的Map和Reduce任务。这些信息对于调试用户代码中的bug很有帮助。

### 4.9 Counters（计数器）

MapReduce库使用计数器（counter）统计不同事件发生次数。例如，用户可能想要统计已经处理过的单词的数目或者德语文件的索引数量。为了使用这一特性，用户代码创建一个命名的counter对象，并且在*Map*以及*Reduce*函数中相应地增加counter的值。例如：

```c++
Counter* uppercase;
uppercase = GetCounter("uppercase");

map(String name, String contents):
	for each word w in contents:
		if (IsCapitalized(w)):
			uppercase->Increment();
		EmitIntermediate(w, "1");
```

这些计数器的值周期性的从各个单独的worker机器上传递给master（附加在ping的应答包中传递）。master把执行成功的Map和Reduce任务的计数器值进行累计，当MapReduce操作完成之后，返回给用户代码。

计数器当前的值也会显示在master的状态页面上，这样用户就可以看到当前计算的进度。当累加计数器的值的时候，master要检查重复运行的Map或者Reduce任务，避免重复累加（之前提到的备用任务和失效后重新执行任务这两种情况会导致相同的任务被多次执行）。

有些计数器的值是由MapReduce库自动维持的，比如已经处理的输入的key/value键值对的数目以及已经产生输出的key/value键值对的数目。

计数器机制对于MapReduce操作的完整性检查非常有用。例如，在有些MapReduce操作中，用户代码想要确保产生的输出对的数目和已经处理的输入对的数目是恰好相等的，或者处理的德语文件的数目占总处理文件数目的比重在一个可容忍的范围内。

## 5 Performance（性能）

本节我们在一个大型集群上运行的两个计算来衡量MapReduce的性能。一个计算在大约1TB的数据中进行特定的模式匹配，另一个计算对大约1TB的数据进行排序。

这两个程序在大量的使用MapReduce的实际应用中是非常典型的——一类是对数据格式进行转换，从一种表现形式转换为另外一种表现形式；另一类是从海量数据中抽取少部分的用户感兴趣的数据。

### 5.1 Cluster Configuration（集群配置）

所有程序都运行在一个由1800台机器组成的机器上。每一台机器都有两个2 GHz 的Intel Xeon处理器，并且Hyper-Threading打开， 4 GB内存，两个160 GB的IDE磁盘和一个千兆以太网卡。这些机器部署在一个两层的树形交换网络中，在root节点大概有100-200GBPS的传输带宽。所有这些机器都采用相同的部署（对等部署），因此任意两点之间的网络来回时间小于1毫秒。

其中4 GB中的1-1.5 G是为集群中运行的其他任务预留的。程序在一个周末的下午运行，此时CPU，磁盘，网络基本都处于空闲状态。

### 5.2 Grep  

这个分布式的grep程序需要扫描$10^{10}$条100-byte的记录，搜索一个相对罕见的三字符模式（出现了92337次）。输入数据被拆分成大约64 M的Block（M = 15000），所有的输出文件都存放在一个文件中（R = 1）。

<img src="https://kdjlyy.com/usr/uploads/2021/05/803991061.png " height="300" />

图2显示了这个运算随时间的处理过程。其中Y轴表示输入数据的处理速度。处理速度随着参与MapReduce计算的机器数量的增加而增加，当1764台worker参与计算的时，处理速度达到了30GB/s。当Map任务结束的时候，即在计算开始后80秒，输入的处理速度降到0。整个计算过程从开始到结束一共花了大概150秒。这包括了大约一分钟的初始启动阶段。初始启动阶段消耗的时间包括了是把这个程序传送到各个worker机器上的时间、等待GFS文件系统打开1000个输入文件集合的时间、获取相关的文件本地位置优化信息的时间。

### 5.3 Sort  

排序程序处理$10^{10}$条100-byte的记录（大概1 TB的数据），这个程序模仿TeraSort benchmark。

*sort*程序由不到50行代码组成。只有三行的*Map*函数从文本行中解析出10个字节的key值作为排序的key，并且把这个key和原始文本行作为中间的key/value值输出。我们使用了一个内置的恒等函数作为*Reduce*操作函数。这个函数把中间的key/value值不作任何改变输出。最终排序结果输出到两路复制的GFS文件系统（也就是说，程序输出2TB的数据）。

如前所述，输入数据被分成64 MB的Block（M = 15000）。我们把排序后的输出结果分区后存储到4000个文件（R = 4000）。分区函数使用key的原始字节来把数据分区到R个片段中。

在这个benchmark测试中，我们使用的分区函数知道key的分区情况。通常对于排序程序来说，我们会增加一个预处理的MapReduce操作用于采样key值的分布情况，通过采样的数据来计算对最终排序处理的分区点。

<img src="https://kdjlyy.com/usr/uploads/2021/05/2573356857.png" style="zoom:100%;" />

图3(a)显示了这个排序程序的正常执行过程。左上的图显示了输入数据读取的速度。数据读取速度峰值会达到13 GB/s，并且所有Map任务完成之后，即大约200秒之后迅速滑落到0。值得注意的是，排序程序输入数据读取速度小于分布式*grep*程序。这是因为排序程序的Map任务花了大约一半的处理时间和I/O带宽把中间输出结果写到本地硬盘。相应的分布式*grep*程序的中间结果输出几乎可以忽略不计。

左边中间的图显示了中间数据从Map任务发送到Reduce任务的网络速度。这个过程从第一个Map任务完成之后就开始缓慢启动了。图示的第一个高峰是启动了第一批大概1700个Reduce任务（整个MapReduce分布到大概1700台机器上，每台机器1次最多执行1个Reduce任务）。排序程序运行大约300秒后，第一批启动的Reduce任务有些完成了，我们开始执行剩下的Reduce任务。所有的处理在大约600秒后结束。

左下图表示Reduce任务把排序后的数据写到最终的输出文件的速度。在第一个排序阶段结束和数据开始写入磁盘之间有一个小的延时，这是因为worker机器正在忙于排序中间数据。磁盘写入速度在2-4 GB/s持续一段时间。输出数据写入磁盘大约持续850秒。计入初始启动部分的时间，整个运算消耗了891秒。这个速度和TeraSort benchmark的最高纪录1057秒相差不多。 

还有一些值得注意的现象：输入数据的读取速度比排序速度和输出数据写入磁盘速度要高不少，这是因为我们的输入数据本地化优化策略起了作用——绝大部分数据都是从本地硬盘读取的，从而节省了网络带宽。排序速度比输出数据写入到磁盘的速度快，这是因为输出数据写了两份（我们使用了2路的GFS文件系统，写入复制节点的原因是为了保证数据可靠性和可用性）。我们把输出数据写入到两个复制节点是因为这是底层文件系统的保证数据可靠性和可用性的实现机制。如果底层文件系统使用纠删码（erasure coding ）的方式而不是复制的方式保证数据的可靠性和可用性，那么在输出数据写入磁盘的时候，就可以降低网络带宽的使用。

### 5.4 Effect of Backup Tasks（备用任务的影响）

图3(b)显示了关闭了备用任务后排序程序执行情况。执行的过程和图3(a)很相似，除了输出数据写磁盘的动作在时间上拖了一个很长的尾巴，而且在这段时间里，几乎没有什么写入动作。在960秒后，只有5个Reduce任务没有完成。这些拖后腿的任务又执行了300秒才完成。整个计算消耗了1283秒，多了44%的执行时间。

### 5.5 Machine Failures（失效的机器）

在图3(c)中演示的排序程序执行的过程中，我们在程序开始后几分钟故意kill了1746个worker中的200个。集群底层的调度立刻在这些机器上重新开始新的worker处理进程（因为只是worker机器上的处理进程被kill了，机器本身还在工作）。

图3(c)显示出了一个“负”的输入数据读取速度，这是因为一些已经完成的Map任务丢失了（由于相应的执行Map任务的worker进程被kill了），需要重新执行这些任务。相关Map任务很快就被重新执行了。整个运算在933秒内完成，包括了初始启动时间（只比正常执行多消耗了5%的时间）。

## 6 Experience

我们在2003年1月完成了第一个版本的MapReduce库，在2003年8月的版本有了显著的增强，这包括了输入数据本地优化、worker机器之间的动态负载均衡等等。从那以后，我们惊喜的发现，MapReduce库能广泛应用于我们日常工作中遇到的各类问题。它现在在Google内部各个领域得到广泛应用，包括：

- 大规模机器学习问题
- Google News和Froogle产品的集群问题
- 从公众查询产品（比如Google的Zeitgeist）的报告中抽取数据。
- 从大量的新应用和新产品的网页中提取有用信息（比如，从大量的位置搜索网页中抽取地理位置信息）。
- 大规模的图形计算

<img src="https://kdjlyy.com/usr/uploads/2021/05/3231369756.png " height="300" />

图4显示了在我们的源代码管理系统中，随着时间推移，独立的MapReduce程序数量的显著增加。从2003年早些时候的0个增长到2004年9月份的差不多900个不同的程序。MapReduce的成功取决于采用MapReduce库能够在不到半个小时时间内写出一个简单的程序，这个简单的程序能够在上千台机器的组成的集群上做大规模并发处理，这极大的加快了开发和原形设计的周期。另外，采用MapReduce库，可以让完全没有分布式或并行系统开发经验的程序员很容易的利用大量的资源，开发出分布式或并行处理的应用。

<img src="https://kdjlyy.com/usr/uploads/2021/05/3382917081.png" height="350"/>

在每个任务结束的时候，MapReduce库统计计算资源的使用状况。在表1，我们列出了2004年8月份MapReduce运行的任务所占用的相关资源。

### 6.1 Large-Scale Indexing（大规模索引）

到目前为止，MapReduce最成功的应用就是重写了Google网络搜索服务所使用到的index系统。索引系统的输入数据是网络爬虫抓取回来的海量的文档，这些文档数据都保存在GFS文件系统里。这些文档原始内容的大小超过了20TB。索引程序是通过一系列的MapReduce操作（大约5到10次）来建立索引。使用MapReduce（替换上一个特别设计的、分布式处理的索引程序）带来这些好处：

- 实现索引部分的代码简单、小巧、容易理解，因为对于容错、分布式以及并行计算的处理都是MapReduce库提供的。比如，使用MapReduce库，计算的代码行数从原来的3800行C++代码减少到大概700行代码。
- MapReduce库的性能已经足够好了，因此我们可以把在概念上不相关的计算步骤分开处理，而不是混在一起以期减少数据传递的额外消耗。概念上不相关的计算步骤的隔离也使得我们可以很容易改变索引处理方式。比如，对之前的索引系统的一个小更改可能要耗费好几个月的时间，但是在使用MapReduce的新系统上，这样的更改只需要花几天时间就可以了。
- 索引系统的操作管理更容易了。因为由机器失效、机器处理速度缓慢、以及网络的瞬间阻塞等引起的绝大部分问题都已经由MapReduce库解决了，不再需要操作人员的介入了。另外，我们可以通过在索引系统集群中增加机器的简单方法提高整体处理性能。

## 7 Related Work

很多系统都提供了严格的编程模式，并且通过对编程的严格限制来实现并行计算。例如，一个结合函数可以通过把$N$个元素的数组的前缀在$N$个处理器上使用并行前缀算法，在$log N$的时间内完成计算。MapReduce可以看作是我们结合在真实环境下处理海量数据的经验，对这些经典模型进行简化和提取的成果。更加值得骄傲的是，我们还实现了基于上千台处理器的集群的容错处理。相比而言，大部分并发处理系统都只在小规模的集群上实现，并且把容错处理交给了程序员。

Bulk Synchronous Programming和一些MPI原语提供了更高级别的并行处理抽象，可以更容易写出并行处理的程序。MapReduce和这些系统的关键不同之处在于，MapReduce利用限制性编程模式实现了用户程序的自动并发处理，并且提供了透明的容错处理。

我们数据本地优化策略的灵感来源于active disks等技术，在active disks中，计算任务是尽量推送到数据存储的节点处理*（注：即靠近数据源处理），*这样就减少了网络和IO子系统的吞吐量。我们在挂载几个硬盘的普通机器上执行我们的运算，而不是在磁盘处理器上执行我们的工作，但是达到的目的一样的。

我们的备用任务机制和Charlotte System提出的Eager调度机制比较类似。Eager调度机制的一个缺点是如果一个任务反复失效，那么整个计算就不能完成。我们通过忽略引起故障的记录的方式在某种程度上解决了这个问题。

MapReduce的实现依赖于一个内部的集群管理系统，这个集群管理系统负责在一个超大的、共享机器的集群上分布和运行用户任务。虽然这个不是本论文的重点，但是有必要提一下，这个集群管理系统在理念上和其它系统，如Condor是一样。

MapReduce库的排序机制和NOW-Sort的操作上很类似。读取输入源的机器（map workers）把待排序的数据进行分区后，发送到R个Reduce worker中的一个进行处理。每个Reduce worker在本地对数据进行排序（尽可能在内存中排序）。当然，NOW-Sort没有给用户自定义的Map和Reduce函数的机会，因此不具备MapReduce库广泛的实用性。

River提供了一个编程模型：处理进程通过分布式队列传送数据的方式进行互相通讯。和MapReduce类似，River系统尝试在不对等的硬件环境下，或者在系统颠簸的情况下也能提供近似平均的性能。River是通过精心调度硬盘和网络的通讯来平衡任务的完成时间。MapReduce库采用了其它的方法。通过对编程模型进行限制，MapReduce框架把问题分解成为大量的“小”任务。这些任务在可用的worker集群上动态的调度，这样快速的worker就可以执行更多的任务。通过对编程模型进行限制，我们可用在工作接近完成的时候调度备用任务，缩短在硬件配置不均衡的情况下缩小整个操作完成的时间（比如有的机器性能差、或者机器被某些操作阻塞了）。

BAD-FS采用了和MapReduce完全不同的编程模式，它是面向广域网的。不过，这两个系统有两个基础功能很类似。（1）两个系统采用重新执行的方式来防止由于失效导致的数据丢失。（2）两个都使用数据本地化调度策略，减少网络通讯的数据量。

TACC是一个用于简化构造高可用性网络服务的系统。和MapReduce一样，它也依靠重新执行机制来实现的容错处理。

## 8 Conclusions

MapReduce编程模型在Google内部成功应用于多个领域。我们把这种成功归结为几个方面：首先，由于MapReduce封装了并行处理、容错处理、数据本地化优化、负载均衡等等技术难点的细节，这使得MapReduce库易于使用。即便对于完全没有并行或者分布式系统开发经验的程序员而言；其次，大量不同类型的问题都可以通过MapReduce简单的解决。比如，MapReduce用于生成Google的网络搜索服务所需要的数据、用来排序、用来数据挖掘、用于机器学习，以及很多其它的系统；第三，我们实现了一个在数千台计算机组成的大型集群上灵活部署运行的MapReduce。这个实现使得有效利用这些丰富的计算资源变得非常简单，因此也适合用来解决Google遇到的其他很多需要大量计算的问题。

我们也从MapReduce开发过程中学到了不少东西。首先，约束编程模式使得并行和分布式计算非常容易，也易于构造容错的计算环境；其次，网络带宽是稀有资源。大量的系统优化是针对减少网络传输量为目的的：本地优化策略使大量的数据从本地磁盘读取，中间文件写入本地磁盘、并且只写一份中间文件也节约了网络带宽。第三，多次执行相同的任务可以减少性能缓慢的机器带来的负面影响，同时解决了由于机器失效导致的数据丢失问题。



[1]: https://kdjlyy.com/usr/uploads/2021/05/834405130.pdf
[2]: https://kdjlyy.com/usr/uploads/2021/05/1747110744.png
[3]: https://kdjlyy.com/usr/uploads/2021/05/803991061.png
[4]: https://kdjlyy.com/usr/uploads/2021/05/2573356857.png
[5]: https://kdjlyy.com/usr/uploads/2021/05/3231369756.png
[6]: https://kdjlyy.com/usr/uploads/2021/05/3382917081.png